{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7b3KGgph6MkR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rdEuYw5L35cP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import VivitImageProcessor, VivitForVideoClassification\n",
        "import random\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "593975c3684e477b8f8401d8d571f06d",
            "e969f77d7eb1440d92e1268a2df1a37f",
            "82fdbb9fbfd0490f9bec2d5edd0fa789",
            "c58d5e8298cb432aa9f67cad091498ed",
            "292a7e5a68094eb98ca3353dcff5c95e",
            "57568d0f8b844d2a9de59b973f2cf656",
            "cc72ba0cbcef40ce8d9c82432487f7b7",
            "a46e31db691642d49fe5eed5dc1c4ad1",
            "c70ad00e07e743b694a4ea19fcf8366f",
            "de740fe7dc004b6c8d0b7398747e19c9",
            "22947679820b45cb8a9322acb563e384"
          ]
        },
        "id": "SYjCRV0t4ACj",
        "outputId": "a06247f1-5455-4341-ff27-e4cac5339f52"
      },
      "outputs": [],
      "source": [
        "model_ckpt = \"google/vivit-b-16x2-kinetics400\"\n",
        "image_processor = VivitImageProcessor.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21midtGG4NiP",
        "outputId": "44a558dc-b265-4b12-ff17-20138e45c5c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique classes: ['down', 'same', 'up'].\n"
          ]
        }
      ],
      "source": [
        "class_labels =['down','same','up']\n",
        "label2id = {label: i for i, label in enumerate(class_labels)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "print(f\"Unique classes: {list(label2id.keys())}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oTWFWNJV5nLY",
        "outputId": "094d48ea-a3dd-45f4-bfd8-be6fdf903027"
      },
      "outputs": [],
      "source": [
        "def get_file_name_and_parent_folder(file_path):\n",
        "  file_path = os.path.normpath(file_path)\n",
        "  file_paths = file_path.split(os.sep)\n",
        "  file_name, file_extension = os.path.splitext(os.path.basename(file_path))\n",
        "  parent_folder = os.path.dirname(file_path)\n",
        "  video_name = file_paths[-2]\n",
        "  player_id, session_id=video_name.split('_solid_')\n",
        "  return file_name, parent_folder, player_id, session_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16zZty8r1etd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class MyCSVDataset(Dataset):\n",
        "    def __init__(self, csv_file, csv_file_2):\n",
        "        \n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.gf = pd.read_csv(csv_file_2)\n",
        "        # revome control, label, and str features in clean_data\n",
        "        self.gf = self.gf.drop(columns=['[control]genre','[control]game', '[control]time_index','[output]arousal','[string]key_presses','[string]player_aim_target','[string]bot_damaged_by'],)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx]\n",
        "        gf = self.gf\n",
        "        clip_path = sample['start_frame']\n",
        "        file_name, parent_folder, player_id, session_id= get_file_name_and_parent_folder(clip_path)\n",
        "        file_name = int(file_name)\n",
        "\n",
        "        start=sample['start_time']\n",
        "        start = int(start)*4\n",
        "        end = start+24\n",
        "\n",
        "        # transform to tensor\n",
        "        game_vactor = gf[(gf['[control]player_id']==player_id)&(gf['[control]session_id']==session_id)]\n",
        "        game_vactor= game_vactor.drop(columns=['[control]player_id', '[control]session_id'],)\n",
        "        \n",
        "        pd.set_option('future.no_silent_downcasting', True)\n",
        "        game_vactor= game_vactor.fillna(0).infer_objects(copy=False)\n",
        "        game_vactor = game_vactor.iloc[start:end]\n",
        "        game_array = np.array(game_vactor.values)\n",
        "        game_tensor = torch.from_numpy(game_array)\n",
        "        game_tensor = game_tensor.float()\n",
        "\n",
        "        frames=[]\n",
        "        for i in range(32):\n",
        "            if (file_name<10):\n",
        "                frame_path = parent_folder + \"/000\" + str(file_name)+ \".png\"\n",
        "            elif (10<=file_name<100):\n",
        "                frame_path = parent_folder + \"/00\" + str(file_name)+ \".png\"\n",
        "            else:\n",
        "                frame_path = parent_folder + \"/0\" + str(file_name)+ \".png\"\n",
        "\n",
        "            # the frames path, adjust it if needed\n",
        "            frame_path = \"../Dataset/\" + frame_path\n",
        "            frame_path = os.path.normpath(frame_path)\n",
        "          \n",
        "            frame = Image.open(frame_path).convert('RGB')\n",
        "            frames.append(frame)\n",
        "            file_name += 1\n",
        "\n",
        "        inputs = image_processor(list(frames), return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values']\n",
        "        pixel_values = pixel_values.squeeze(0)\n",
        "        inputs['pixel_values'] = pixel_values\n",
        "\n",
        "        label=sample['arousal_change']\n",
        "        label=label2id[label]\n",
        "        label_numpy = np.array([label])\n",
        "        label_tensor = torch.from_numpy(label_numpy)\n",
        "        label_tensor=torch.LongTensor(label_tensor)\n",
        "        \n",
        "        inputs['label']=label_tensor\n",
        "        inputs['game_tensor'] = game_tensor \n",
        "\n",
        "        return inputs\n",
        "\n",
        "# path to the helper file and clean_data file, adjust it if needed\n",
        "csv_file = '../Dataset/new_solid.csv'\n",
        "csv_file_2 = \"../Dataset/clean_data.csv\"\n",
        "dataset = MyCSVDataset(csv_file,csv_file_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eKFfTWq2hKe"
      },
      "source": [
        "Split train and test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YrYyJ10226h7"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset, [10240, 2560]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am8hyhjz29Vp"
      },
      "source": [
        "dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_GHjNnUs21jn"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataloader = DataLoader(test_dataset, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlUc7JmR3FXv"
      },
      "source": [
        "Training or finetuning helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aSqUEgaQ25NH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# plot helper\n",
        "def plot(loss_list, output_path):\n",
        "    plt.figure(figsize=(10,5))\n",
        "\n",
        "    freqs = [i for i in range(len(loss_list))]\n",
        "    # Plotting training loss curves\n",
        "    plt.plot(freqs, loss_list, color='#e4007f', label=\"train/loss curve\")\n",
        "\n",
        "    # Plotting axes and legends\n",
        "    plt.ylabel(\"loss\", fontsize='large')\n",
        "    plt.xlabel(\"epoch\", fontsize='large')\n",
        "    plt.legend(loc='upper right', fontsize='x-large')\n",
        "\n",
        "    plt.savefig(output_path+'/pytorch_vivit_loss_curve.png')\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dVfHfUx3O_p"
      },
      "source": [
        "# Training or finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Au0oRfR73QgN"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "# load finetuned vivit.(finetuned_vivit_10epochs (finetuned_vivit_1) or finetuned_vivit_15epochs(finetuned_vivit_2))\n",
        "# adjust the model path if needed\n",
        "\n",
        "video_encoder = VivitForVideoClassification.from_pretrained(\n",
        "    \"TrainedModels/finetuned_vivit_15epochs\",\n",
        "    label2id = label2id,\n",
        "    id2label = id2label,\n",
        "    ignore_mismatched_sizes = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Freeze the paremeters in finetuned vivit.\n",
        "\n",
        "for param in video_encoder.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MLP GCF Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defined the MLP GF encoder\n",
        "class GfEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GfEncoder, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(24*112, 1024),  \n",
        "            nn.ReLU(),  \n",
        "            nn.Linear(1024, 512),  \n",
        "            nn.ReLU(),  \n",
        "            nn.Linear(512, 768),  \n",
        "        )\n",
        "        \n",
        "        # Xavier init\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # adjust shape\n",
        "        x = self.layers(x)\n",
        "        return x.view(x.size(0), -1)  # Adjust the shape of the output tensor to [batch_size, 768]\n",
        "    \n",
        "\n",
        "gf_encoder = GfEncoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overall contrastive model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ContrastiveModel(nn.Module):\n",
        "    def __init__(self, gf_model, video_model):\n",
        "        super(ContrastiveModel, self).__init__()\n",
        "        self.gf_model = gf_model\n",
        "        self.video_model = video_model\n",
        "\n",
        "        # learnable temperature\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]))\n",
        "\n",
        "        \n",
        "\n",
        "     \n",
        "\n",
        "    def forward(self, gf_inputs, video_inputs):\n",
        "        gf_outputs = self.gf_model(gf_inputs)  # game context feature representation output from GF encoder \n",
        "        video_outputs = self.video_model(video_inputs,output_hidden_states=True)  # game footage videos representation output from video encoder \n",
        "        video_outputs = video_outputs.hidden_states[-1][:, 0, :]\n",
        "\n",
        "        # normalize\n",
        "        gf_outputs = gf_outputs / gf_outputs.norm(dim=1, keepdim=True)\n",
        "        video_outputs = video_outputs / video_outputs.norm(dim=1, keepdim=True)\n",
        "        \n",
        "        # cosine similarity as logits\n",
        "        logit_scale = self.logit_scale.exp()  \n",
        "        logits_per_video = logit_scale * video_outputs @ gf_outputs.t()  # similarity\n",
        "        logits_per_gf = logits_per_video.t()  \n",
        "\n",
        "    \n",
        "        return gf_outputs, video_outputs ,logits_per_video, logits_per_gf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "model=ContrastiveModel(gf_encoder,video_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyDwo_c13Zqp"
      },
      "outputs": [],
      "source": [
        "# lr, epoch can be changed\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_size=len(train_dataset)\n",
        "model.to(device)\n",
        "model.train()\n",
        "loss_list = []\n",
        "for epoch in range(11):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    sum_loss_list = []\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "        gf = batch.pop(\"game_tensor\").to(device)\n",
        "\n",
        "        gf_outputs, video_outputs, logits_per_video, logits_per_gf = model(gf_inputs= gf,video_inputs=pixel_values)\n",
        "\n",
        "        # generate contrastive learning label\n",
        "        labels = torch.arange(logits_per_video.size(0), device=logits_per_video.device)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = (\n",
        "            F.cross_entropy(logits_per_video, labels) +\n",
        "            F.cross_entropy(logits_per_gf, labels)\n",
        "        ) / 2\n",
        "\n",
        "\n",
        "        print(\"Epoch:\",epoch,\" , idx:\",idx,\" , Loss:\", loss.item())\n",
        "        sum_loss_list.append(float(loss.item()))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    avg_sum_loss = sum(sum_loss_list)/len(sum_loss_list)\n",
        "    print(\"epoch: \", epoch, \"loss: \", float(avg_sum_loss))\n",
        "    loss_list.append(float(avg_sum_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHJqJLFR3m_X"
      },
      "outputs": [],
      "source": [
        "# the path to save model,adjust if needed\n",
        "model_id =\"./TrainedModels/contrastive_15epoch\"\n",
        "\n",
        "if not os.path.exists(model_id):\n",
        "    os.makedirs(model_id)\n",
        "model_file = 'model.pt'\n",
        "print(\"model_output:\", model_id)\n",
        "torch.save(model, os.path.join(model_id, model_file))\n",
        "plot(loss_list, model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "add classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load trained model\n",
        "contrastive_model = torch.load(os.path.join(model_id, 'model.pt'))\n",
        "\n",
        "# Freeze the parameters of the model\n",
        "for param in contrastive_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model with classifier layer\n",
        "class ContrastiveForClassification(nn.Module):\n",
        "    def __init__(self, ContrastiveModel,num_classes):\n",
        "        super(ContrastiveForClassification, self).__init__()\n",
        "        self.ContrastiveModel = ContrastiveModel\n",
        "        self.classifier = nn.Linear(in_features=1536, out_features=num_classes)\n",
        "    \n",
        "\n",
        "    def forward(self, gf_inputs, video_inputs):\n",
        "        # get the representations from GF encoder and video encoder after contrastive learning \n",
        "        gf_outputs, video_outputs, logits_per_video, logits_per_gf = self.ContrastiveModel(gf_inputs, video_inputs)\n",
        "\n",
        "        # concatenate gf_outputs and video_outputs \n",
        "        x = torch.cat([gf_outputs, video_outputs], dim=1)\n",
        "        x = self.classifier(x)      \n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model = ContrastiveForClassification(contrastive_model,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train classifier layer, lr and epoch can be changed\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_size=len(train_dataset)\n",
        "model.to(device)\n",
        "model.train()\n",
        "loss_list = []\n",
        "for epoch in range(11):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    sum_loss_list = []\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device)        \n",
        "        gf = batch.pop(\"game_tensor\").to(device)\n",
        "        label = batch.pop(\"label\").to(device)\n",
        "\n",
        "        outputs = model(gf_inputs= gf,video_inputs=pixel_values)\n",
        "        label = label.squeeze(1)\n",
        "\n",
        "        loss = criterion(outputs, label)\n",
        "        print(\"Epoch:\",epoch,\" , idx:\",idx,\" , Loss:\", loss.item())\n",
        "        sum_loss_list.append(float(loss.item()))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_sum_loss = sum(sum_loss_list)/len(sum_loss_list)\n",
        "    print(\"epoch: \", epoch, \"loss: \", float(avg_sum_loss))\n",
        "    loss_list.append(float(avg_sum_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_path =\"./TrainedModels/contrastive_15epochs_classification\"\n",
        "model_id = output_path\n",
        "\n",
        "if not os.path.exists(model_id):\n",
        "    os.makedirs(model_id)\n",
        "model_file = 'model.pt'\n",
        "print(\"model_output:\", model_id)\n",
        "torch.save(model, os.path.join(model_id, model_file))\n",
        "plot(loss_list, model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test\n",
        "correct = 0\n",
        "total = 0\n",
        "pred_result = []\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, batch in enumerate(test_dataloader):\n",
        "      pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "      gf = batch.pop(\"game_tensor\").to(device)\n",
        "\n",
        "      label = batch.pop(\"label\").to(device)\n",
        "      outputs = model(gf_inputs= gf,video_inputs=pixel_values)\n",
        "      label = label.squeeze(1)\n",
        "\n",
        "      predicted_id = outputs.argmax(-1).item()\n",
        "      pred_result.append([idx,predicted_id])\n",
        "      total += label.size(0)\n",
        "      correct += (predicted_id == label).sum().item()\n",
        "acc = correct / total\n",
        "print(\"accuracy:\",acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('acc.txt', 'a') as f:\n",
        "    f.write('acc_contrastive_15epoch_classification:\\n')\n",
        "    f.write(str(acc))\n",
        "    f.write('\\n')\n",
        "\n",
        "print(\"successfully recorded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training MLP GCF encoder (directly train on game features from scratch using supervised learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GfEncoder_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GfEncoder_2, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(24*112, 1024), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(1024, 512), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(512, 768), \n",
        "        )\n",
        "        self.classifier = nn.Linear(in_features=768, out_features=3)\n",
        "        \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  \n",
        "        representation = self.layers(x)\n",
        "        x = self.classifier(representation)\n",
        "        return representation, x  \n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GfEncoder_2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_size=len(train_dataset)\n",
        "model.to(device)\n",
        "model.train()\n",
        "loss_list = []\n",
        "for epoch in range(11):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    sum_loss_list = []\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "      \n",
        "        gf = batch.pop(\"game_tensor\").to(device)\n",
        "        label = batch.pop(\"label\").to(device)\n",
        "\n",
        "        representations, outputs = model(gf)\n",
        "        label = label.squeeze(1)\n",
        "\n",
        "        loss = criterion(outputs, label)\n",
        "\n",
        "        print(\"Epoch:\",epoch,\" , idx:\",idx,\" , Loss:\", loss.item())\n",
        "        sum_loss_list.append(float(loss.item()))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_sum_loss = sum(sum_loss_list)/len(sum_loss_list)\n",
        "    print(\"epoch: \", epoch, \"loss: \", float(avg_sum_loss))\n",
        "    loss_list.append(float(avg_sum_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_path =\"./TrainedModels/MLP_classification_supervise\"\n",
        "model_id = output_path\n",
        "\n",
        "if not os.path.exists(model_id):\n",
        "    os.makedirs(model_id)\n",
        "model_file = 'model.pt'\n",
        "print(\"model_output:\", model_id)\n",
        "torch.save(model, os.path.join(model_id, model_file))\n",
        "plot(loss_list, model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "pred_result = []\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, batch in enumerate(test_dataloader):\n",
        "     \n",
        "      gf = batch.pop(\"game_tensor\").to(device)\n",
        "\n",
        "      label = batch.pop(\"label\").to(device)\n",
        "      representations, outputs = model(gf)\n",
        "      label = label.squeeze(1)    \n",
        "      print('output shape: ',outputs.size())\n",
        "      predicted_id = outputs.argmax(-1).item()\n",
        "      pred_result.append([idx,predicted_id])\n",
        "      total += label.size(0)\n",
        "      correct += (predicted_id == label).sum().item()\n",
        "acc = correct / total\n",
        "print(\"accuracy:\",acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('acc.txt', 'a') as f:\n",
        "    f.write('MLP_classification_supervise:\\n')\n",
        "    f.write(str(acc))\n",
        "    f.write('\\n')\n",
        "\n",
        "print(\"successfully recorded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Only use representations from GCF encoder trained by using contrastive learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {},
      "outputs": [],
      "source": [
        "contrastive_model = torch.load(os.path.join('./TrainedModels/contrastive_15epoch', 'model.pt'))\n",
        "\n",
        "for param in contrastive_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContrastiveMLPForClassification(nn.Module):\n",
        "    def __init__(self, ContrastiveModel,num_classes):\n",
        "        super(ContrastiveMLPForClassification, self).__init__()\n",
        "        self.ContrastiveModel = ContrastiveModel\n",
        "        self.classifier = nn.Linear(in_features=768, out_features=num_classes)\n",
        "    \n",
        "\n",
        "    def forward(self, gf_inputs, video_inputs):\n",
        "        \n",
        "        gf_outputs, video_outputs, logits_per_video, logits_per_gf = self.ContrastiveModel(gf_inputs, video_inputs)\n",
        "\n",
        "        x = self.classifier(gf_outputs)     \n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ContrastiveMLPForClassification(contrastive_model,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "train_size=len(train_dataset)\n",
        "model.to(device)\n",
        "model.train()\n",
        "loss_list = []\n",
        "for epoch in range(6):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    sum_loss_list = []\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "\n",
        "        pixel_values = batch.pop(\"pixel_values\").to(device)        \n",
        "        gf = batch.pop(\"game_tensor\").to(device)\n",
        "        label = batch.pop(\"label\").to(device)\n",
        "\n",
        "        outputs = model(gf_inputs=gf, video_inputs=pixel_values)\n",
        "        label = label.squeeze(1)\n",
        "\n",
        "        loss = criterion(outputs, label)\n",
        "\n",
        "        print(\"Epoch:\",epoch,\" , idx:\",idx,\" , Loss:\", loss.item())\n",
        "        sum_loss_list.append(float(loss.item()))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_sum_loss = sum(sum_loss_list)/len(sum_loss_list)\n",
        "    print(\"epoch: \", epoch, \"loss: \", float(avg_sum_loss))\n",
        "    loss_list.append(float(avg_sum_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_path =\"./TrainedModels/Contrastive_15epoch_MLP_only_classification\"\n",
        "model_id = output_path\n",
        "\n",
        "if not os.path.exists(model_id):\n",
        "    os.makedirs(model_id)\n",
        "model_file = 'model.pt'\n",
        "print(\"model_output:\", model_id)\n",
        "torch.save(model, os.path.join(model_id, model_file))\n",
        "plot(loss_list, model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "pred_result = []\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, batch in enumerate(test_dataloader):\n",
        "      pixel_values = batch.pop(\"pixel_values\").to(device)        \n",
        "      gf = batch.pop(\"game_tensor\").to(device)\n",
        "\n",
        "      label = batch.pop(\"label\").to(device)\n",
        "      outputs = model(gf_inputs=gf, video_inputs=pixel_values)\n",
        "      label = label.squeeze(1)\n",
        "      \n",
        "      print('output: ',outputs)\n",
        "      print('label: ',label)\n",
        "      \n",
        "      predicted_id = outputs.argmax(-1).item()\n",
        "      pred_result.append([idx,predicted_id])\n",
        "      total += label.size(0)\n",
        "      correct += (predicted_id == label).sum().item()\n",
        "acc = correct / total\n",
        "print(\"accuracy:\",acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('acc.txt', 'a') as f:\n",
        "    f.write('Contrastive_15epoch_MLP_only_classification:\\n')\n",
        "    f.write(str(acc))\n",
        "    f.write('\\n')\n",
        "\n",
        "print(\"successfully recorded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract GCF encoder from trained contrastive model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 535,
      "metadata": {},
      "outputs": [],
      "source": [
        "gf_encoder_part = contrastive_model.ContrastiveModel.gf_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 536,
      "metadata": {},
      "outputs": [],
      "source": [
        "gf_classifier = contrastive_model.classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContrastiveMLPForClassification_2(nn.Module):\n",
        "    def __init__(self, encoder,classifier):\n",
        "        super(ContrastiveMLPForClassification_2, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = classifier\n",
        "    \n",
        "\n",
        "    def forward(self, gf_inputs):\n",
        "        \n",
        "        gf_inputs = gf_inputs.view(gf_inputs.size(0), -1) \n",
        "        gf_outputs= self.encoder(gf_inputs)\n",
        "\n",
        "        x = self.classifier(gf_outputs)\n",
        "      \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ContrastiveMLPForClassification_2(gf_encoder_part,gf_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "pred_result = []\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, batch in enumerate(test_dataloader):      \n",
        "      gf = batch.pop(\"game_tensor\").to(device)\n",
        "\n",
        "      label = batch.pop(\"label\").to(device)\n",
        "      outputs = model(gf_inputs=gf)\n",
        "      label = label.squeeze(1)   \n",
        "\n",
        "      predicted_id = outputs.argmax(-1).item()\n",
        "      pred_result.append([idx,predicted_id])\n",
        "      total += label.size(0)\n",
        "      correct += (predicted_id == label).sum().item()\n",
        "acc = correct / total\n",
        "print(\"accuracy:\",acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('acc.txt', 'a') as f:\n",
        "    f.write('Contrastive_15epoch_extracted_MLP_classification:\\n')\n",
        "    f.write(str(acc))\n",
        "    f.write('\\n')\n",
        "\n",
        "print(\"successfully recorded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "22947679820b45cb8a9322acb563e384": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "292a7e5a68094eb98ca3353dcff5c95e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57568d0f8b844d2a9de59b973f2cf656": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "593975c3684e477b8f8401d8d571f06d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e969f77d7eb1440d92e1268a2df1a37f",
              "IPY_MODEL_82fdbb9fbfd0490f9bec2d5edd0fa789",
              "IPY_MODEL_c58d5e8298cb432aa9f67cad091498ed"
            ],
            "layout": "IPY_MODEL_292a7e5a68094eb98ca3353dcff5c95e"
          }
        },
        "82fdbb9fbfd0490f9bec2d5edd0fa789": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a46e31db691642d49fe5eed5dc1c4ad1",
            "max": 430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c70ad00e07e743b694a4ea19fcf8366f",
            "value": 430
          }
        },
        "a46e31db691642d49fe5eed5dc1c4ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c58d5e8298cb432aa9f67cad091498ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de740fe7dc004b6c8d0b7398747e19c9",
            "placeholder": "​",
            "style": "IPY_MODEL_22947679820b45cb8a9322acb563e384",
            "value": " 430/430 [00:00&lt;00:00, 24.8kB/s]"
          }
        },
        "c70ad00e07e743b694a4ea19fcf8366f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc72ba0cbcef40ce8d9c82432487f7b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de740fe7dc004b6c8d0b7398747e19c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e969f77d7eb1440d92e1268a2df1a37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57568d0f8b844d2a9de59b973f2cf656",
            "placeholder": "​",
            "style": "IPY_MODEL_cc72ba0cbcef40ce8d9c82432487f7b7",
            "value": "preprocessor_config.json: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
